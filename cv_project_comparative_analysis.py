# -*- coding: utf-8 -*-
"""CV_Project_Comparative_Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19eulvIz9SCj7ZHuRDValL8buAltlmiFF

# **IMPORTING DRIVE**
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **IMPORTING ESSENTIAL LIBRARIES**"""

import numpy as np
import matplotlib.pyplot as plt
import os
import cv2

import pandas as pd
from sklearn import preprocessing
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

import pandas as pd

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

import numpy as np
import matplotlib.pyplot as plt

data = {'VGG16':66.79,'EfficientNet B7':71.29, 'ResNet-50':67.42, 'Inception v3 ': 63.37, 'EfficientNet B7 with improved dataset':78.07}
courses = list(data.keys())
values = list(data.values())

fig = plt.figure(figsize = (13, 7))

# creating the bar plot
plt.bar(courses, values, color ='blue',
        width = 0.4)

plt.xlabel("Model used")
plt.ylabel("Accuracy")
plt.title("Result")
plt.show()

"""# **LOADING DATASET**"""

DATADIR = "/content/drive/MyDrive/ai_project/train/train_img"
i=0
im_size = 224

train_images = []
for img in os.listdir (DATADIR) :
  img_array = cv2.imread(os.path. join(DATADIR, img))
  img_array = cv2.resize(img_array, (im_size, im_size))
  img_array = np.array(img_array / 255.0)
  train_images.append(img_array)
  print(i)
  if i >= 2000:
    break
  i = i + 1

DATADIR2 = "/content/drive/MyDrive/ai_project/test_img"
i=0
im_size = 224

test_images = []
for img in os.listdir (DATADIR2) :
  img_array = cv2.imread(os.path. join(DATADIR2, img))
  img_array = cv2.resize(img_array, (im_size, im_size))
  img_array = np.array(img_array / 255.0)
  test_images.append(img_array)
  print(i)
  if i >= 800:
    break
  i = i+ 1

train_annot = pd.read_csv('/content/drive/MyDrive/ai_project/train/annot_train.txt',sep='\t')
train_annot.head(10)

test_annot = pd.read_csv('/content/drive/MyDrive/ai_project/annot_test.txt',sep='\t')
test_annot.head(10)

"""# **PREPROCESSING DATASET**"""

train_images

train_images=np.array(train_images)

train_images.shape

test_images

test_images = np.array(test_images)

test_images.shape

y=train_annot['protest'].values
y=y.reshape(-1,1)
y=y[:2001]

z=test_annot['protest'].values
z=z.reshape(-1,1)
z=z[:801]

c_train=0
for i in y:
  if i==1 :
    c_train+=1

c_test=0
for i in z:
  if i==1 :
    c_test+=1

c_train

c_test

y=train_annot['protest'].values
print(y)
y_labelencoder = LabelEncoder ()
y = y_labelencoder.fit_transform (y)
print (y)

y=y.reshape(-1,1)
print(y)

from sklearn.compose import ColumnTransformer
ct = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')
Y = ct.fit_transform(y) #.toarray()
print(Y[:5])
print(Y[35:])

z=test_annot['protest'].values
print(z)
z_labelencoder = LabelEncoder ()
z = z_labelencoder.fit_transform (z)
print (z)

z=z.reshape(-1,1)
ct2 = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')
Z = ct.fit_transform(z) #.toarray()
print(Z[:5])
print(Z[35:])

train_annot = train_annot[:2001]
test_annot = test_annot[:801]
Y=Y[:2001]
Z=Z[:801]

print(train_images.shape)
print(Y.shape)
print(test_images.shape)
print(Z.shape)

import tensorflow as tf

config = tf.compat.v1.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.3
tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))

train_images = train_images.astype('float32')

tf.convert_to_tensor(
    train_images, dtype=None, dtype_hint=None, name=None
)

tf.convert_to_tensor(
    Y, dtype=None, dtype_hint=None, name=None
)

test_images = test_images.astype('float32')

tf.convert_to_tensor(
    test_images, dtype=None, dtype_hint=None, name=None
)

tf.convert_to_tensor(
    Z, dtype=None, dtype_hint=None, name=None
)

"""# **COMPARITIVE ANALYSIS**

"""

import cv2
import numpy as np
from matplotlib.pyplot import imread
from matplotlib.pyplot import imshow
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.imagenet_utils import decode_predictions
from tensorflow.keras.applications.imagenet_utils import preprocess_input
from tensorflow.keras import layers
from tensorflow.keras.applications import EfficientNetB7
import numpy as np
import tensorflow as tf

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os


NUM_CLASSES = 2
IMG_SIZE = 224

"""**EFFICIENTNET B7**"""

def build_model(num_classes):
    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    x = inputs
    model = EfficientNetB7(include_top=False, input_tensor=x, weights="imagenet")
    model.trainable = False
    x = layers.GlobalAveragePooling2D(name="avg_pool")(model.output)
    x = layers.BatchNormalization()(x)

    top_dropout_rate = 0.2
    x = layers.Dropout(top_dropout_rate, name="top_dropout")(x)
    outputs = layers.Dense(NUM_CLASSES, activation="softmax", name="pred")(x)

    # Compile
    model = tf.keras.Model(inputs, outputs, name="EfficientNet")
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)
    model.compile(
        optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"]
    )
    return model

model = build_model(num_classes=NUM_CLASSES)

import matplotlib.pyplot as plt

def plot_hist(hist):
    plt.plot(hist.history["accuracy"])
    #plt.plot(hist.history["val_accuracy"])
    plt.title("model accuracy")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.show()

epochs = 10
hist = model.fit(train_images, Y , epochs=epochs, verbose=2)

preds = model.evaluate(test_images, Z)
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

"""**ACCURACY FROM EFFICIENTNET B7 = 71.29%**"""

plot_hist(hist)

"""We observe from the plot that the model is giving best accuracy at 5 epochs."""

p1='/content/protest-01-as-gty-200619_hpMain_16x9_992.jpg'
p2='/content/th.jfif'

from PIL import Image
im=Image.open(p1)
display(im)

from PIL import Image
im=Image.open(p2)
display(im)

y=[]
img_array = cv2.imread(p1)
img_array = cv2.resize(img_array, (im_size, im_size))
img_array = np.array(img_array / 255.0)
y.append(img_array)
img_array = cv2.imread(p2)
img_array = cv2.resize(img_array, (im_size, im_size))
img_array = np.array(img_array / 255.0)
y.append(img_array)

y=np.array(y)

y.shape

y_pred = model.predict(y)

y_pred

hist_e = model.fit(train_images, Y , epochs=5, verbose=2, batch_size=32,validation_split=0.2)

y_pred = model.predict(test_images)

y_pred_class = [np.argmax(element) for element in y_pred]
rounded_labels=np.argmax(Z, axis=1)
rounded_labels[1]
print(classification_report(rounded_labels, y_pred_class))

"""Accuracy: 83%"""

plt.plot(hist_e.history['accuracy'])
plt.plot(hist_e.history['val_accuracy'])
plt.ylim( [ 0, 1 ] )
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(hist_e.history['loss'])
plt.plot(hist_e.history['val_loss'])
plt.ylim( [ 0, 3 ] )
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""From the model accuracy and model loss graph we observe that our model is under fitted. We see that the differnce between the train and test accuracy is very less. Variance is low. Bias is High. We need to build a more complex model for achieving a better accuracy.

**RESNET50**
"""

from tensorflow.keras.applications import ResNet50

model2 = ResNet50(input_shape=(224, 224,3), include_top=False, weights="imagenet")

for layer in model2.layers:
    layer.trainable = False

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D

model2 = Sequential()
model2.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))
model2.add(Dense(2, activation='sigmoid'))

from keras import optimizers

model2.compile(
   optimizer='sgd',
   loss = 'binary_crossentropy',
   metrics = ['acc']
)

resnet_history = model2.fit(train_images, Y, steps_per_epoch = 30, epochs = 50)

preds = model2.evaluate(test_images, Z)
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

"""**ACCURACY FROM RESNET50  = 67.42%**

**VGG-16**
"""

from tensorflow.keras.applications.vgg16 import VGG16

model3 = VGG16(input_shape = (224, 224, 3), # Shape of our images
include_top = False, # Leave out the last fully connected layer
weights = 'imagenet')

for layer in model3.layers:
    layer.trainable = False

x = layers.Flatten()(model3.output)

# Add a fully connected layer with 512 hidden units and ReLU activation
x = layers.Dense(512, activation='relu')(x)

# Add a dropout rate of 0.5
x = layers.Dropout(0.5)(x)

# Add a final sigmoid layer with 1 node for classification output
x = layers.Dense(1, activation='sigmoid')(x)

model3 = tf.keras.models.Model(model3.input, x)

model3.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001), loss = 'binary_crossentropy',metrics = ['acc'])

vgghist = model3.fit(train_images, y, steps_per_epoch = 30, epochs = 50)

preds = model3.evaluate(test_images, z)
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

"""**ACCURACY FROM VGG-16  = 66.79%**

**InceptionV3**
"""

from tensorflow.keras.applications.inception_v3 import InceptionV3
model4 = InceptionV3(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')

for layer in model4.layers:
    layer.trainable = False

from tensorflow.keras.optimizers import RMSprop

x = layers.Flatten()(model4.output)
x = layers.Dense(1024, activation='relu')(x)
x = layers.Dropout(0.2)(x)

x = layers.Dense(1, activation='sigmoid')(x)

model4 = tf.keras.models.Model(model4.input, x)

model4.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])

inc_history = model4.fit(train_images , y,  epochs = 50)

preds = model4.evaluate(test_images, z)
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

"""**ACCURACY FROM InceptionV3  = 63.67%**

**CONCLUSION:**

Accuracy of InceptionV3     - 63.67% ;
Accuracy of VGG-16          - 66.79% ;
Accuracy of ResNet50        - 67.42% ;
Accuracy of EfficientNetB7  - 71.29%

**EfficientNetB7 has the best accuracy.**

# **Making dataset more uniform**
"""

DATADIR = "/content/drive/MyDrive/ai_project/train/train_img"
train_annot = pd.read_csv('/content/drive/MyDrive/ai_project/train/annot_train.txt',sep='\t')
y1=train_annot['protest'].values

i=0
k=0
im_size = 224

train_images2 = []
for img in os.listdir (DATADIR) :
  if y1[i]==1:
    img_array = cv2.imread(os.path. join(DATADIR, img))
    img_array = cv2.resize(img_array, (im_size, im_size))
    img_array = np.array(img_array / 255.0)
    train_images2.append(img_array)
    k=k+1
    print(i)
  if k>=1000:
    break
  i = i + 1

i=0
k=0
im_size = 224
for img in os.listdir (DATADIR) :
  if y1[i]==0:
    img_array = cv2.imread(os.path. join(DATADIR, img))
    img_array = cv2.resize(img_array, (im_size, im_size))
    img_array = np.array(img_array / 255.0)
    train_images2.append(img_array)
    k=k+1
    print(i)
  if k>=1000:
    break
  i = i + 1

DATADIR2 = "/content/drive/MyDrive/ai_project/test_img"
test_annot = pd.read_csv('/content/drive/MyDrive/ai_project/annot_test.txt',sep='\t')
z1=test_annot['protest'].values
i=0
k=0
im_size = 224

test_images2 = []
for img in os.listdir (DATADIR2) :
  if z1[i]==1:
    img_array = cv2.imread(os.path. join(DATADIR2, img))
    img_array = cv2.resize(img_array, (im_size, im_size))
    img_array = np.array(img_array / 255.0)
    test_images2.append(img_array)
    k=k+1
    print(i)
  if k>=400:
    break
  i = i + 1

i=0
k=0
im_size = 224

for img in os.listdir (DATADIR2) :
  if z1[i]==0:
    img_array = cv2.imread(os.path. join(DATADIR2, img))
    img_array = cv2.resize(img_array, (im_size, im_size))
    img_array = np.array(img_array / 255.0)
    test_images2.append(img_array)
    k=k+1
    print(i)
  if k>=400:
    break
  i = i + 1

y1 = np.zeros(shape = (2000,1), dtype = int)
for i in range(2000):
  if(i<1000):
    y1[i]=1
  else:
    y1[i]=0

y1.size

y1

z1 = np.zeros(shape = (800,1), dtype = int)
i=0
for i in range(800):
  if(i<400):
    z1[i]=1
  else:
    z1[i]=0

z1

z1.size

train_images2=np.array(train_images2)

train_images2.shape

test_images2=np.array(test_images2)

test_images2.shape

from sklearn.compose import ColumnTransformer
ct = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')
y1 = ct.fit_transform(y1) #.toarray()

y1

z1 = ct.fit_transform(z1) #.toarray()

z1

a = list(zip(train_images2, y1))

import random
random.shuffle(a)

train_images2, y1 = zip(*a)

train_images2=np.asarray(train_images2)

y1=np.asarray(y1)

a = list(zip(test_images2, z1))

random.shuffle(a)

test_images2, z1= zip(*a)

z1

test_images2=np.asarray(test_images2)

z1=np.asarray(z1)

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Dense, Activation, Flatten

cnn = Sequential()
cnn.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=(224, 224, 3)))
cnn.add(Conv2D(4, (3, 3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2, 2)))
cnn.add(Conv2D(8, kernel_size=(3, 3),activation='relu',input_shape=(224, 224, 3)))
cnn.add(Conv2D(4, (3, 3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2, 2)))
cnn.add(Conv2D(4, kernel_size=(3, 3),activation='relu',input_shape=(224, 224, 3)))
cnn.add(Conv2D(4, (3, 3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2, 2)))
cnn.add(Conv2D(8, kernel_size=(3, 3),activation='relu',input_shape=(224, 224, 3)))
cnn.add(Conv2D(4, (3, 3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2, 2)))
cnn.add(Flatten())
cnn.add(Dense(25, activation='relu'))
cnn.add(Dense(50, activation='tanh'))
cnn.add(Dense(40, activation='relu'))
cnn.add(Dense(2, activation='sigmoid'))

cnn.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')

history_cnn = cnn.fit(train_images2, y1,batch_size=32,validation_split=0.2, epochs=10)

plt.plot(history_cnn.history['accuracy'])
plt.plot(history_cnn.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_cnn.history['loss'])
plt.plot(history_cnn.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

y_pred = cnn.predict(test_images2)

y_pred_classes = [np.argmax(element) for element in y_pred]
rounded_labels=np.argmax(z1, axis=1)
rounded_labels[1]
print(classification_report(rounded_labels, y_pred_classes))

acc2 = cnn.evaluate(test_images2, z1)
print ("Test Accuracy = " + str(acc2[1]))

hist_e1 = model.fit(train_images2, y1 , epochs=5, verbose=2, batch_size=32,validation_split=0.2)

y_pred = model.predict(test_images2)

y_pred

y_pred.shape

y_pred_class = [np.argmax(element) for element in y_pred]

rounded_labels=np.argmax(z1, axis=1)
rounded_labels

print(classification_report(rounded_labels, y_pred_class))

plt.plot(hist_e1.history['accuracy'])
plt.plot(hist_e1.history['val_accuracy'])
plt.ylim( [ 0, 1 ] )
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'validation'], loc='upper left')
plt.show()

plt.plot(hist_e1.history['loss'])
plt.plot(hist_e1.history['val_loss'])
plt.ylim( [ 0, 3 ] )
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

preds = model.evaluate(test_images2, z1)
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

hist_r1 = model2.fit(train_images2, y1 , epochs=5, verbose=2, batch_size=32,validation_split=0.2,steps_per_epoch = 3)

preds = model2.evaluate(test_images2, z1)
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

y_pred = model2.predict(test_images2)
y_pred_class = [np.argmax(element) for element in y_pred]
rounded_labels=np.argmax(z1, axis=1)
print(classification_report(rounded_labels, y_pred_class))

plt.plot(hist_r1.history['acc'])
plt.plot(hist_r1.history['val_acc'])
plt.ylim( [ 0, 1 ] )
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'validation'], loc='upper left')
plt.show()

plt.plot(hist_r1.history['loss'])
plt.plot(hist_r1.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""stratified sampling of training data into train and validation"""

from sklearn.model_selection import train_test_split
X_train, X_v, y_train, y_v = train_test_split(train_images2, y1,stratify=y1,test_size=0.25)

hist_e3 = model.fit(X_train, y_train ,validation_data=(X_v,y_v) ,epochs=5, verbose=2, batch_size=32)

preds = model.evaluate(test_images2, z1)
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

y_pred = model.predict(test_images2)
y_pred_class = [np.argmax(element) for element in y_pred]
rounded_labels=np.argmax(z1, axis=1)
print(classification_report(rounded_labels, y_pred_class))

plt.plot(hist_e3.history['accuracy'])
plt.plot(hist_e3.history['val_accuracy'])
plt.ylim( [ 0, 1 ] )
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'validation'], loc='upper left')
plt.show()

plt.plot(hist_e3.history['loss'])
plt.plot(hist_e3.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

def build_model(num_classes):
    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    x = inputs
    model = EfficientNetB7(include_top=False, input_tensor=x, weights="imagenet")
    model.trainable = False
    x = layers.GlobalAveragePooling2D(name="avg_pool")(model.output)
    x = layers.BatchNormalization()(x)

    top_dropout_rate = 0.2
    x = layers.Dropout(top_dropout_rate, name="top_dropout")(x)
    outputs = layers.Dense(NUM_CLASSES, activation="sigmoid", name="pred")(x)

    # Compile
    model = tf.keras.Model(inputs, outputs, name="EfficientNet")
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)
    model.compile(
        optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"]
    )
    return model

model = build_model(num_classes=NUM_CLASSES)

model.summary()

hist_en = model.fit(train_images2, y1 , epochs=10, verbose=2, batch_size=32,validation_split=0.2)

preds = model.evaluate(test_images2, z1)
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

y_pred = model.predict(test_images2)
y_pred

y_pred = model.predict(test_images2)
y_pred_class = [np.argmax(element) for element in y_pred]
rounded_labels=np.argmax(z1, axis=1)
print(classification_report(rounded_labels, y_pred_class))



"""new try"""

def build_model(num_classes):
    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    x = inputs
    outputs = EfficientNetB7(include_top=True, weights=None, classes=NUM_CLASSES)(x)

    model = tf.keras.Model(inputs, outputs)
    model.compile(
        optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"]
    )
    return model

model.summary()

hist_ent = model.fit(train_images2, y1 , epochs=7, verbose=2, batch_size=32,validation_split=0.2)

preds = model.evaluate(test_images2, z1)
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

y_pred = model.predict(test_images2)

y_pred = model.predict(test_images2)
y_pred_class = [np.argmax(element) for element in y_pred]
rounded_labels=np.argmax(z1, axis=1)
print(classification_report(rounded_labels, y_pred_class))